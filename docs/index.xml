<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>学飞的猪</title>
    <link>/</link>
    <description>Recent content on 学飞的猪</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright © 2021-{year} YinpengChen. All Rights Reserved.</copyright>
    <lastBuildDate>Tue, 26 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用Pytorch进行单机多卡分布式训练</title>
      <link>/posts/%E4%BD%BF%E7%94%A8pytorch%E8%BF%9B%E8%A1%8C%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/%E4%BD%BF%E7%94%A8pytorch%E8%BF%9B%E8%A1%8C%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</guid>
      <description>一. torch.nn.DataParallel ? pytorch单机多卡最简单的实现方法就是使用nn.DataParallel类，其几乎仅使用一行代码net = torch.nn.Dat</description>
    </item>
    
  </channel>
</rss>
